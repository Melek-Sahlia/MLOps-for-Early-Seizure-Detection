{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZhqO4Fmzhs"
      },
      "source": [
        "* Team Name: DSTune +\n",
        "\n",
        "* Email : malek.Sahlia@ensi-uma.tn\n",
        "* LinkedIn : https://www.linkedin.com/in/melek-sahlia/\n",
        "* Challenge: AI Night Challenge 24 -\n",
        "MLOps pour la Détection des Débuts de Crises Épileptiques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_x4XbYPCxc-N"
      },
      "outputs": [],
      "source": [
        "#!git clone https://gitlab.com/eps_mlops/defi_ai_night_mlops.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aeHtsBt1tKZ0",
        "outputId": "8e424089-bc84-4e18-be11-0a932323bc2c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Défi: MLOps pour la Détection des Débuts de Crises Épileptiques\n",
              "\n",
              "## Description du Défi\n",
              "\n",
              "Le défi consiste à développer un modèle de détection du début des crises épileptiques en se basant sur l'analyse de signaux physiologiques. Les participants sont appelés à utiliser les meilleures pratiques de MLOps pour développer, déployer et gérer leur modèle de manière efficace et reproductible. La solution proposée devra inclure un processus d'extraction de features à partir des signaux physiologiques, permettant de capturer des informations pertinentes pour la détection des crises. Le modèle devra être capable de classifier les états épileptiques afin de détecter le début des crises avec une précision élevée, tout en minimisant les faux positifs et les faux négatifs. En outre, il est demandé de concevoir une interface utilisateur permettant aux médecins de visualiser les détections.\n",
              "\n",
              "[Lien original de  la base CHB-MIT Scalp EEG Database.](https://physionet.org/content/chbmit/1.0.0/chb01/#files-panel)\n",
              "\n",
              "## Description du Pipeline\n",
              "\n",
              "Pour créer un pipeline de manière efficace et reproductible, les participants doivent utiliser ZenML. Voici les étapes du pipeline:\n",
              "\n",
              "1. **Initialisation du dépôt ZenML :** Initialisation du référentiel ZenML pour gérer le code et les artefacts du pipeline.\n",
              "   \n",
              "2. **Définition de la source de données CSV :** Définition de la source de données pour lire les ensembles de données CSV, telles que la base de données CHB-MIT Scalp EEG déjà préparée.\n",
              "   \n",
              "3. **Fractionnement des données :** Division des données en ensembles de formation et de validation pour l'évaluation du modèle.\n",
              "   \n",
              "4. **Construction du pipeline ZenML :** Création du pipeline à l'aide de ZenML pour le traitement des données et la construction de modèles.\n",
              "   \n",
              "5. **Formation en pipeline :** Entraînement du pipeline pour entraîner le modèle à l’aide de données préparées.\n",
              "   \n",
              "6. **Évaluation du modèle :** Validation des performances du modèle sur l’ensemble par rapport à une baseline établie.\n",
              "   \n",
              "7. **Exportation du modèle :** Exportation du modèle entraîné pour un déploiement ultérieur sur une interface utilisateur pour la visualisation des détections.\n",
              "\n",
              "N.B. MLFlow peut également être utilisé avec ou à la place de ZenML. Voici les liens vers les documentations :\n",
              "- [Documentation de ZenML](https://docs.zenml.io/)\n",
              "- [Documentation de MLFlow](https://mlflow.org/docs/latest/index.html)\n",
              "\n",
              "## Description du Dataset\n",
              "\n",
              "La classification des crises d'épilepsie est une tâche cruciale dans le domaine médical, notamment dans le diagnostic et la prise en charge des patients atteints d'épilepsie. Dans le contexte de l'électroencéphalographie (EEG), la classification des crises d'épilepsie vise à identifier les périodes de l'activité électrique cérébrale qui correspondent à une crise épileptique par rapport aux périodes d'activité électrique normale.\n",
              "\n",
              "Le CHB-MIT Scalp EEG Database est une ressource précieuse dans le domaine de la recherche sur l'épilepsie. Il s'agit d'une collection d'enregistrements EEG de sujets pédiatriques présentant des crises épileptiques réfractaires. Ces enregistrements ont été effectués dans le cadre d'une surveillance prolongée après l'arrêt de la médication antiépileptique, dans le but de caractériser les crises et d'évaluer l'éligibilité des patients à une intervention chirurgicale. Au total, 182 crises ont été annotées, fournissant une base de données riche en données pour étudier les modèles d'activité cérébrale associés aux crises épileptiques chez les enfants.\n",
              "\n",
              "Le processus de classification peut être réalisé en utilisant des techniques d'apprentissage automatique, où les caractéristiques extraites des signaux EEG sont utilisées pour distinguer entre les états de crise épileptique et les états normaux.\n",
              "\n",
              "Les états peuvent être étiquetés comme suit :\n",
              "- 0 (EEG interictal, cerveau normal) : Cette étiquette est associée aux périodes où l'EEG enregistre une activité électrique cérébrale normale, en dehors des périodes de crises épileptiques. Pendant ces périodes, le patient ne présente pas de symptômes d'épilepsie et son cerveau fonctionne dans des limites normales.\n",
              "- 1 (Crise d'épilepsie) : Cette étiquette est associée aux périodes où l'EEG enregistre une activité électrique cérébrale caractéristique des crises épileptiques. Ces périodes sont marquées par des modèles d'activité électrique anormaux, tels que des décharges épileptiformes, des pointes ou des ondes lentes.\n",
              "\n",
              "La classification des crises d'épilepsie a plusieurs implications cliniques importantes :\n",
              "- Diagnostic précoce et précis\n",
              "- Suivi de l'évolution de la maladie\n",
              "- Optimisation du traitement\n",
              "- Prévention des crises\n",
              "\n",
              "Le processus d'extraction de caractéristiques revêt une importance cruciale dans la classification des crises d'épilepsie à partir des enregistrements EEG, permettant la réduction de la dimensionnalité des données, la capture de motifs caractéristiques pertinents, l'amélioration de la séparabilité des classes, la robustesse aux artefacts et au bruit, ainsi que la facilitation de l'interprétation des résultats pour mieux comprendre la pathophysiologie de l'épilepsie.\n",
              "\n",
              "Les participants doivent utiliser les datasets disponibles dans le GitLab du défi pour mener à bien leur travail.\n",
              "\n",
              "Voici une Description textuelle de chaque caractéristique dans Dataset :\n",
              "1. DFA (Detrended Fluctuation Analysis) : Analyse des fluctuations détrendues de la série temporelle.\n",
              "2. Fisher Information : Mesure de l'information de Fisher, une quantité liée à la capacité d'un processus stochastique à transporter des informations sur un paramètre inconnu.\n",
              "3. HFD (Higuchi Fractal Dimension) : Dimension fractale de Higuchi, une mesure de la complexité fractale d'une série temporelle.\n",
              "4. PFD (Petrosian Fractal Dimension) : Dimension fractale de Petrosian, une mesure de la rugosité ou de la complexité fractale d'une série temporelle.\n",
              "5. SVD (Singular Value Decomposition) Entropy : Entropie de la décomposition en valeurs singulières, une mesure de la complexité de la série temporelle basée sur la décomposition en valeurs singulières.\n",
              "6. Variance : Variance de la série temporelle, mesurant la dispersion des valeurs par rapport à la moyenne.\n",
              "7. Écart type : Mesure de la dispersion des valeurs par rapport à la moyenne, similaire à la variance mais en unités de la série temporelle.\n",
              "8. Moyenne : Moyenne arithmétique de la série temporelle, représentant la tendance centrale des données.\n",
              "9. Variance de la transformée de Fourier (FFT) : Variance des coefficients de la transformée de Fourier rapide (FFT), mesurant la dispersion des fréquences présentes dans le signal.\n",
              "10. Écart type de la transformée de Fourier (FFT) : Mesure de la dispersion des coefficients de la transformée de Fourier rapide (FFT) par rapport à leur moyenne.\n",
              "11. Variance de la deuxième transformée de Fourier (FFT2) : Variance des coefficients de la deuxième transformée de Fourier, fournissant des informations supplémentaires sur les fréquences présentes dans le signal.\n",
              "12. Taux de passage par zéro : Nombre de passages à travers zéro dans la série temporelle, indiquant les changements de direction dans le signal.\n",
              "13. Complexité : Mesure de la complexité du signal, basée sur des critères tels que l'irrégularité et la variabilité.\n",
              "\n",
              "--- \n",
              "\n",
              "## Edit: Clarification des canaux\n",
              "\n",
              "- Ch0x fait référence au nom du patient.\n",
              "- Le channel (canal) 1 ou 2 est la source du signal EEG.\n",
              "\n",
              "Dans les enregistrements EEG, chaque canal représente un emplacement spécifique d'électrode sur le cuir chevelu. La convention de dénomination des canaux suit généralement un système standardisé, tel que le système 10-20, où la désignation « Ch0x » fait référence au nom du patient et « Canal 1 » ou « Canal 2 » correspond à la source du signal EEG. enregistré à partir des électrodes respectives. Ces canaux fournissent des informations spatiales sur l’activité cérébrale, permettant aux chercheurs d’analyser les signaux électriques provenant de différentes régions du cerveau."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Path to the README.md file\n",
        "file_path = '/content/defi_ai_night_mlops/README.md'\n",
        "\n",
        "# Reading the content of the file\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Displaying the content as Markdown\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nAJGoywUl2ar"
      },
      "outputs": [],
      "source": [
        "#!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sqOMuOFnxr-2"
      },
      "outputs": [],
      "source": [
        "#!pip install zenml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZjBHcxHJ_HaK"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Tuple\n",
        "from xgboost import XGBClassifier\n",
        "from zenml import pipeline, step\n",
        "import shap  # Assuming you need to use SHAP analysis later on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBce3lZXS8bA",
        "outputId": "c0698bde-d1fd-4674-aa40-5578632e8ee6"
      },
      "outputs": [],
      "source": [
        "%pip install \"zenml[server]\"\n",
        "%pip install zenml\n",
        "!zenml integration install sklearn -y\n",
        "%pip install pyparsing\n",
        "import IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvVlKu3gTIMl",
        "outputId": "9722b5a0-a164-4e36-c9da-00856e7e1b6f"
      },
      "outputs": [],
      "source": [
        "NGROK_TOKEN = \"NGROK_TOKEN\" #Once registred at NGROK, You will be provided with an access Token\n",
        "from zenml.environment import Environment\n",
        "\n",
        "if Environment.in_google_colab():\n",
        "  !pip install pyngrok\n",
        "  !ngrok authtoken {NGROK_TOKEN}\n",
        "\n",
        "!rm -rf .zen\n",
        "!zenml init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2Sj4sZi3b3Bf"
      },
      "outputs": [],
      "source": [
        "# Define the pattern to match the files\n",
        "pattern = '/content/defi_ai_night_mlops/ch??_eeg_features_label.csv'\n",
        "\n",
        "# Use glob to find all files matching the pattern\n",
        "file_paths = glob.glob(pattern)\n",
        "\n",
        "# Initialize a list to hold DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Loop through the file paths, reading each file into a DataFrame\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Use regex to extract the patient number from the filename\n",
        "    patient_number = re.search(r'ch(\\d+)_eeg', file_path)\n",
        "    if patient_number:\n",
        "        patient_id = int(patient_number.group(1))\n",
        "    else:\n",
        "        patient_id = -1  # Example default value\n",
        "\n",
        "    # Insert the 'patient' column as the first column\n",
        "    df.insert(0, 'patient', patient_id)\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame, if needed\n",
        "final_df = pd.concat(dfs, ignore_index=True)\n",
        "final_df.to_csv('final_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sMUcyBwzPhVO"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def import_epilepsy_data() -> pd.DataFrame:\n",
        "  df=pd.read_csv(\"final_df.csv\")\n",
        "  df.info()\n",
        "  nb=df['patient'].nunique()\n",
        "  print(f\"We have {nb}  patients in epilepsy_data.\")\n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "s3v83AKao10F"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def split_and_balance_data(df: pd.DataFrame, test_size: float = 0.2, valid_size: float = 0.3, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits the data into train, validation, and test sets, and balances the training set.\n",
        "\n",
        "    :param df: Input DataFrame.\n",
        "    :param test_size: Proportion of the dataset to include in the test split.\n",
        "    :param valid_size: Proportion of the dataset to include in the validation split, from the non-test split.\n",
        "    :param random_state: Random state for reproducibility.\n",
        "    :return: A tuple containing balanced training, validation, and test DataFrames.\n",
        "    \"\"\"\n",
        "\n",
        "    # Shuffling and resetting the index of the dataframe\n",
        "    df_data = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    # Splitting the data into validation/test and training sets\n",
        "    df_valid_test = df_data.sample(frac=test_size, random_state=random_state)\n",
        "    df_train_all = df_data.drop(df_valid_test.index)\n",
        "\n",
        "    # Further splitting the validation/test set into validation and test sets\n",
        "    df_test = df_valid_test.sample(frac=0.5, random_state=random_state)\n",
        "    df_valid = df_valid_test.drop(df_test.index)\n",
        "\n",
        "    # Balancing the training set\n",
        "    rows_pos = df_train_all.label == 1\n",
        "    df_train_pos = df_train_all.loc[rows_pos]\n",
        "    df_train_neg = df_train_all.loc[~rows_pos]\n",
        "    n = min(len(df_train_pos), len(df_train_neg))\n",
        "\n",
        "    df_train_balanced = pd.concat([\n",
        "        df_train_pos.sample(n=n, random_state=random_state),\n",
        "        df_train_neg.sample(n=n, random_state=random_state)\n",
        "    ], ignore_index=True).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    # Returning the balanced training set, validation set, and test set\n",
        "    return df_train_all, df_train_balanced, df_valid, df_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1Gp84dGN_soh"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def prepare_and_scale_data(df_train_all: pd.DataFrame, df_train: pd.DataFrame,df_valid: pd.DataFrame,  random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]:\n",
        "\n",
        "    cols_input = list(df_train_all.columns)\n",
        "    cols_input.remove('patient')\n",
        "    cols_input.remove('label')\n",
        "\n",
        "    # Create the feature matrices and target vectors\n",
        "    X_train = df_train[cols_input].values\n",
        "    X_train_all = df_train_all[cols_input].values\n",
        "    X_valid = df_valid[cols_input].values\n",
        "\n",
        "    y_train = df_train['label'].values\n",
        "    y_valid = df_valid['label'].values\n",
        "\n",
        "    print('Training All shapes:', X_train_all.shape)\n",
        "    print('Training shapes:', X_train.shape, y_train.shape)\n",
        "    print('Validation shapes:', X_valid.shape, y_valid.shape)\n",
        "\n",
        "    # Scale the feature matrices\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train_all)  # Fit the scaler using the full training dataset\n",
        "\n",
        "    # Save the scaler to disk\n",
        "    scalerfile = 'scaler.sav'\n",
        "    pickle.dump(scaler, open(scalerfile, 'wb'))\n",
        "\n",
        "    # Load the scaler from disk (demonstration purposes, typically you would do this in a different part of your workflow)\n",
        "    scaler = pickle.load(open(scalerfile, 'rb'))\n",
        "\n",
        "    # Transform the feature matrices using the loaded scaler\n",
        "    X_train_tf = scaler.transform(X_train)\n",
        "    X_valid_tf = scaler.transform(X_valid)\n",
        "\n",
        "    return X_train_tf, X_valid_tf, y_train, y_valid, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-08T15:13:52.971854Z",
          "start_time": "2019-05-08T15:13:52.951028Z"
        },
        "id": "XAO-4S_H1foA"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def calc_specificity(y_actual: np.ndarray, y_pred: np.ndarray, thresh: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the specificity of a classification model.\n",
        "\n",
        "    Parameters:\n",
        "    - y_actual: np.ndarray. Array of actual labels.\n",
        "    - y_pred: np.ndarray. Array of predicted probabilities.\n",
        "    - thresh: float. Probability threshold for classifying predictions.\n",
        "\n",
        "    Returns:\n",
        "    - Specificity score as a float.\n",
        "    \"\"\"\n",
        "    true_negatives = sum((y_pred < thresh) & (y_actual == 0))\n",
        "    condition_negatives = sum(y_actual == 0)\n",
        "    specificity = true_negatives / condition_negatives if condition_negatives else 0\n",
        "    return specificity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2nU2KfrOAzvC"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def calc_prevalence(y_actual: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the prevalence of positive cases in a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - y_actual: np.ndarray. Array of actual labels.\n",
        "\n",
        "    Returns:\n",
        "    - Prevalence score as a float.\n",
        "    \"\"\"\n",
        "    return np.mean(y_actual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ktXYgcqLA4fh"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def print_report(y_actual: np.ndarray, y_pred: np.ndarray, thresh: float = 0.5) -> dict:\n",
        "\n",
        "    metrics = {\n",
        "        'AUC': roc_auc_score(y_actual, y_pred),\n",
        "        'Accuracy': accuracy_score(y_actual, (y_pred > thresh)),\n",
        "        'Recall': recall_score(y_actual, (y_pred > thresh)),\n",
        "        'Precision': precision_score(y_actual, (y_pred > thresh)),\n",
        "        'Specificity': calc_specificity(y_actual, y_pred, thresh),\n",
        "        'Prevalence': calc_prevalence(y_actual)\n",
        "    }\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f'{metric}: {value:.3f}')\n",
        "\n",
        "    print(' ')\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Lkn8qXQWAd4i"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "# Corrected function signature with proper type hinting\n",
        "def train_and_evaluate_xgboost(X_train_tf: np.ndarray, y_train: np.ndarray, X_valid_tf: np.ndarray, y_valid: np.ndarray, thresh: float = 0.5) -> Tuple[XGBClassifier, dict, dict]:\n",
        "    \"\"\"\n",
        "    Trains an XGBoost model and evaluates its performance on both training and validation datasets.\n",
        "    \"\"\"\n",
        "    # Model training\n",
        "    xgbc = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    xgbc.fit(X_train_tf, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_preds = xgbc.predict_proba(X_train_tf)[:, 1]\n",
        "    y_valid_preds = xgbc.predict_proba(X_valid_tf)[:, 1]\n",
        "\n",
        "    # Evaluation (assuming print_report is a previously defined function that returns metrics)\n",
        "    print('Xtreme Gradient Boosting Classifier Performance:')\n",
        "    print('Training:')\n",
        "    train_metrics = print_report(y_train, y_train_preds, thresh)  # Ensure print_report is defined or imported\n",
        "\n",
        "    print('Validation:')\n",
        "    valid_metrics = print_report(y_valid, y_valid_preds, thresh)\n",
        "\n",
        "    return xgbc, train_metrics, valid_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6qb0HfzPfkPr"
      },
      "outputs": [],
      "source": [
        "@step\n",
        "def explain_model(df_train: np.ndarray, xgbc: XGBClassifier) -> np.ndarray:\n",
        "\n",
        "    # Define the features matrix X and labels y if necessary\n",
        "    X = df_train.copy()\n",
        "    X = pd.DataFrame(X)\n",
        "\n",
        "    # Initialize SHAP JS visualization\n",
        "    shap.initjs()\n",
        "\n",
        "    # Use the provided XGBoost classifier model\n",
        "    model = xgbc\n",
        "\n",
        "    # Explain the model's predictions using SHAP values\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X)\n",
        "\n",
        "    return shap_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH3UrZhWX87b",
        "outputId": "745a4848-b316-47bf-cdec-55480b7f0c4b"
      },
      "outputs": [],
      "source": [
        "@pipeline\n",
        "def epilepsy_pipeline():\n",
        "    epilepsy_data = import_epilepsy_data()\n",
        "    # Correct expectation of four returned datasets from the split_and_balance_data function\n",
        "    df_train_all, df_train_balanced, df_valid, df_test = split_and_balance_data(epilepsy_data)\n",
        "\n",
        "    # Ensure subsequent functions accept and process these datasets correctly\n",
        "    X_train_tf, X_valid_tf, y_train, y_valid, scaler = prepare_and_scale_data(df_train_all, df_train_balanced, df_valid)\n",
        "    xgbc, train_metrics, valid_metrics = train_and_evaluate_xgboost(X_train_tf, y_train, X_valid_tf, y_valid)\n",
        "    explain_model(X_train_tf, xgbc)\n",
        "\n",
        "epilepsy_pipeline = epilepsy_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLNUamukdCUl",
        "outputId": "20840da5-8d52-454a-f283-be9453599b2a"
      },
      "outputs": [],
      "source": [
        "from zenml.environment import Environment\n",
        "\n",
        "def start_zenml_dashboard(port=8237):\n",
        "  if Environment.in_google_colab():\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "    public_url = ngrok.connect(port)\n",
        "    #print(f\"\\xlb[31mIn Colab, use this url instead: {public_url}!\\xlb]0m\")\n",
        "    print(f\"\\\\x1b[31mIn Colab, use this url instead: {public_url}!\\\\x1b[0m\")\n",
        "    !zenml up --blocking --port {port}\n",
        "\n",
        "  else:\n",
        "    !zenml up --port {port}\n",
        "start_zenml_dashboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5V0grGrQ6B4",
        "outputId": "3112b689-a960-4f34-d04f-90aea51f781e"
      },
      "outputs": [],
      "source": [
        "!zenml down"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
